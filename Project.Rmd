---
title: "Project"
output: html_document
date: "2025-06-20"
---

```{r}
rm(list=ls())
library(rjags)
library(bayesplot)
library(jagsUI)
library(glue)
library(loo)
library(knitr)
```

# Bayesian Learning & Monte-Carlo Simulation final project

## Introduction

Setting up environment, importing the dataset and visualising a graph of the two time series we will have to fit.

```{r}
rm(list=ls())

# New Family Houses Sold: United States
# Source: https://fred.stlouisfed.org/series/HSN1F
data      <- read.csv("gdp_inflation.csv",header=T)
data$DATE <- as.Date(data$DATE)

# Plotting time series for visualisation purposes
plot(
  data$DATE[1:305],
  data$GDP_PC1[1:305],
  type="l",
  xlab="",
  ylab="GDP",
  main="GDP + CPI"
)
lines(
  data$DATE[1:305],
  data$CPIAUCSL_PC1[1:305],
  type="l",
  col="red"
)
```

At this point, we setup our environment by converting the values in both time series into numericals, and then splitting them into training and testing. Finally, we define some useful variables which we will be using through out our script.

```{r}
# Dataset extraction
x_axis      <- data$DATE[1:305]
series_GDP  <- as.numeric(data$GDP_PC1[1:305])
series_CPI  <- as.numeric(data$CPIAUCSL_PC1[1:305])

# Parameter setup (RMK: we assume both time series of same length)
TRAIN_PERC  <- 0.15
N_tot       <- length(series_GDP)
N_test      <- floor(TRAIN_PERC * N_tot)
N_train     <- N_tot - N_test
```

To further inspect our data we can also plot the respective auto-correlation functions:

```{r}
acf(series_GDP, lag.max=100)
acf(series_CPI, lag.max=100)
```

We now define a few utility functions which will be useful later. The first one automatises the interactions with JAGS, the second one will draw plots of both the whole time-series and its out-of-sample portion only.

```{r}
fit_JAGS <- function(
  model_string,   # JAGS model string
  data_list,      # List of input values for JAGS
  to_save,        # Vector with parameter names which should be saved
  n_iter=5000,    # Number of iterations. Should be greater than n_burnin
  n_adapt=1000,   # JAGS n.adapt internal parameter
  n_chain=1,      # Number of chains to run
  n_burnin=1000,  # Burn-in value
  n_thin=1        # Thinning value
) {
  
  output <- jags(
    model.file=textConnection(model_string),
    data=data_list,
    parameters.to.save=to_save,
    n.adapt=n_adapt,
    n.iter=n_iter,
    n.chains=n_chain,
    n.burnin=n_burnin,
    n.thin=n_thin
  )
  
  return(output)
}
```

```{r}
visualise <- function(
  x_axis,           # Values to be displayed on the x-axis of the plots
  original_series,  # Original time series with all samples
  output,           # Output from JAGS, with in-sample and out-of-sample predictions
  title="",         # Title to display on the graphs
  keep=5            # Training sampled to keep in the test graph
) {
  # Length of total series
  N <- length(x_axis)
  
  
  # Plotting entire graph (training-set + predictions)
  # In-sample predicted values with their 2.5%-97.5% quantiles
  yp_in_mean  <- output$mean$yp_in
  yp_in_q1    <- output$q2.5$yp_in
  yp_in_q2    <- output$q97.5$yp_in
  in_length   <- length(yp_in_mean)
  
  # Out-of-sample predicted values with their 2.5%-97.5% quantiles and 25%-75% quantiles
  yp_out_mean <- output$mean$yp_out
  yp_out_q1   <- output$q2.5$yp_out
  yp_out_q2   <- output$q97.5$yp_out
  yp_out_q3   <- output$q25$yp_out
  yp_out_q4   <- output$q75$yp_out
  
  # Removing potentially non-finite values from quantiles
  yp_in_q1[is.infinite(yp_in_q1)]   <- NA
  yp_in_q2[is.infinite(yp_in_q2)]   <- NA
  yp_out_q1[is.infinite(yp_out_q1)] <- NA
  yp_out_q2[is.infinite(yp_out_q2)] <- NA
  yp_out_q3[is.infinite(yp_out_q3)] <- NA
  yp_out_q4[is.infinite(yp_out_q4)] <- NA
  
  # Computing extreme values for first graph
  min_y <- min(original_series, yp_in_q1, yp_out_q1)
  max_y <- max(original_series, yp_in_q2, yp_out_q2)
  
  # Plotting REAL time series (as points only), both training set and test-set
  plot(
    x_axis,
    original_series,
    main=title, col="red",
    xlab="time", ylab="price",
    ylim=c(min_y, max_y)
  )
  lines(
    x_axis,
    original_series,
    col="red"
  )
  
  # Plotting a vertical line signalling end of training set and horizontal
  # Showing the computer mean
  abline(v=x_axis[in_length], col="magenta", lwd=1, lty=2)
  abline(h=output$mean$m0,    col="green",   lwd=1, lty=2)
  
  # Plotting in-sample predictions as well as 2.5%-97.5% quantiles
  points(x_axis[1:in_length], yp_in_mean, col="blue", pch="*")
  lines(x_axis[1:in_length],  yp_in_q1,   col="blue", lwd=1.5)
  lines(x_axis[1:in_length],  yp_in_q2,   col="blue", lwd=1.5)
  
  # Plotting out-of-sample predicted values as well as 2.5%-97.5%
  # quantiles and 25%-75% quantiles
  points(x_axis[(in_length+1):N], yp_out_mean, col="orange", pch="*")
  lines(x_axis[(in_length+1):N],  yp_out_q1,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q2,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q3,   col="orange", lwd=1.5, lty=2)
  lines(x_axis[(in_length+1):N],  yp_out_q4,   col="orange", lwd=1.5, lty=2)
  

  # Plotting onyl out-of-sample portion of graph (with last 5 training samples)
  # Re-computing extreme values for second graph
  min_y <- min(original_series[(in_length+1 - keep):N], yp_out_q1)
  max_y <- max(original_series[(in_length+1 - keep):N], yp_out_q2)
  
  # Plotting REAL time series (as points only), both training set and test-set
  plot(
    x_axis[(in_length+1 - keep):N],
    original_series[(in_length+1 - keep):N],
    main=title, col="red",
    xlab="time", ylab="price",
    ylim=c(min_y, max_y)
  )
  lines(
    x_axis[(in_length+1-keep):N],
    original_series[(in_length+1-keep):N],
    col="red"
  )
  
  # Plotting a vertical line signalling end of training set
  abline(v=x_axis[in_length+1], col="magenta", lwd=1, lty=2)
  abline(h=output$mean$m0,      col="green",   lwd=1, lty=2)
  
  # Plotting out-of-sample predicted values as well as 2.5%-97.5%
  # quantiles and 25%-75% quantiles
  points(x_axis[(in_length+1):N], yp_out_mean, col="orange", pch="*")
  lines(x_axis[(in_length+1):N],  yp_out_q1,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q2,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q3,   col="orange", lwd=1.5, lty=2)
  lines(x_axis[(in_length+1):N],  yp_out_q4,   col="orange", lwd=1.5, lty=2)
}
```

```{r}
# Function in charge of visualising the anomaly points on top of its corresponding time series
visualise_anomalies <- function(
  x_axis,             # X-axis to use for the plot
  original_series,    # Original time series to display
  high_var_indices,   # List of indices corresponding to high variance points
  med_var_indices,    # List of indices corresponding to medium variance points
  title="",           # Name of the time series used
  start_point=1       # Value after which plotting should start (in order to obtain close up graphs)
) {
  # Plot actual time series
  N <- length(x_axis)
  plot(
    x_axis[start:N],
    original_series[start:N],
    main=glue("Anomalies for {title}"),
    t="l", xlab="time", ylab="price"
  )

  # Plotting strong anomaly points
  points(x_axis[high_var_indices], original_series[high_var_indices], col="red")
  for (high_anomaly in high_var_indices) {
    abline(v=x_axis[high_anomaly], col="red", lwd=1.5)
  }
  
  # Plotting medium anomaly points
  points(x_axis[med_var_indices], original_series[med_var_indices], col="orange")
  for (medium_anomaly in med_var_indices) {
    abline(v=x_axis[medium_anomaly], col="orange", lty=3, lwd=1.5)
  }
}
```

```{r}
# Function in charge of classifying anomaly points given their delta value after running the anomaly detection simulation
classify_anomalies <- function(
  output,             # JAGS simulation output
  N_tot,              # Total number of samples
  title="",           # Name of time series to display
  high_value_thr=0.2, # High variance threshold value used
  med_value_thr=0.60  # Medium variance threshold value used
) {
  # Obtaining precision, variance and delta
  tau     <- output$mean$tau
  sigma2  <- 1 / tau
  delta   <- output$mean$d
  
  # Classifying the values of d
  high_var_indices  <- seq(1:(N_tot-1))[delta < high_value_thr]
  med_var_indices   <- seq(1:(N_tot-1))[high_value_thr < delta & delta < med_value_thr]
  low_var_indices   <- seq(1:(N_tot-1))[delta > med_value_thr]
  
  # Logging counts
  n_high  <- length(high_var_indices)
  n_med   <- length(med_var_indices)
  print(
    glue("Found {n_high} strong and {n_med} medium outliers out {N_tot} total samples for {title}")
  )
  
  # Returning the indices
  return(list(
    high_var_indices = high_var_indices,
    med_var_indices  = med_var_indices,
    low_var_indices  = low_var_indices
  ))
}
```

```{r}
# Function in charge of plotting the anomaly point delta values with their relative threshold lines
display_classification <- function(
  x_axis,             # X-axis to use for the plot
  delta,              # Sequence of delta values
  high_var_indices,   # List of indices corresponding to high variance points
  med_var_indices,    # List of indices corresponding to medium variance points
  low_var_indices,    # List of indices corresponding to low variance points
  title="",           # Name of the time series used
  high_value_thr=0.2, # High variance threshold value used
  med_value_thr=0.60  # Medium variance threshold value used
) {
  # Plotting the posterior values of delta and classifying them
  plot(
    x_axis[low_var_indices], delta[low_var_indices],
    xlab="time",
    ylab="Posterior delta values",
    main=glue("{title} anomaly samples"),
    xlim=c(min(x_axis), max(x_axis)),
    ylim=c(0, 1)
  )
  
  points(x_axis[high_var_indices], delta[high_var_indices], col="red")
  points(x_axis[med_var_indices], delta[med_var_indices]  , col="orange")
  abline(h=high_value_thr, col="red")
  abline(h=med_value_thr, col="orange")
}
```

```{r}

# K is the number of effective params
compute_metrics <- function(output, series, N_train, k) {
  
  ############################################################
  ###################### IN-SAMPLE ###########################
  ############################################################
  # In-sample: DIC, WAIC, BIC
  ll_mat <- output$sims.list$loglik
  DIC_val <- compute_dic_marginal(ll_mat)
  
  waic_res <- suppressWarnings( waic(ll_mat) )
  WAIC_val <- waic_res$estimates["waic","Estimate"]
  
  #BIC computation
  #Sum of the log-likelihoods for each draw
  ll_sum <- rowSums(ll_mat)
  
  # maximum of the sum: estimate of log p(y|theta_hat), that is, log-lik_max
  lhat <- max(ll_sum)
  
  # BIC_val <- -2 * lhat + k * log(N_train)
  BIC_val <- -2 * lhat + k * log(N_train - output$mean$train_metrics_skip)
  
  # BIC Approx
  # BIC_val  <- DIC_val + k * (log(N_train) - 2) 
  
  ############################################################
  #################### OUT-OF-SAMPLE #########################
  ############################################################

  y_pred <- if (is.null(output$mean$yp_onestep_out)) output$mean$yp_out else output$mean$yp_onestep_out

  y_true <- series[(N_train+1):length(series)]
  e <- y_pred - y_true
  
  # MSE
  MSE_val <- mean((e)^2)
  
  #MAE
  MAE_val <- mean(abs(e))
  MAPE_val <- mean(abs(e / y_true)) * 100
  
  #SMAPE
  SMAPE_val <- mean(2 * abs(e) / (abs(y_true) + abs(y_pred))) * 100
  
  #MASE
  scale <- mean(abs(diff(series[1:N_train])))
  MASE_val <- MAE_val / scale
  
 list(
    DIC   = round(DIC_val, 1),
    WAIC  = round(WAIC_val, 1),
    BIC   = round(BIC_val, 1),
    MSE   = round(MSE_val, 3),
    MAE   = round(MAE_val, 3),
    MAPE  = round(MAPE_val, 1),
    SMAPE = round(SMAPE_val, 1),
    MASE  = round(MASE_val, 3)
  )
}

compute_dic_marginal <- function(ll_mat) {
  dev      <- -2 * ll_mat
  dev_sum  <- rowSums(dev)
  Dbar     <- mean(dev_sum)
  pD       <- 0.5 * var(dev_sum)
  DIC      <- Dbar + pD
  return(DIC)
}

```

## JAGS model strings

At this point, we define all our JAGS model strings so as to be able to reuse them multiple times through out the script. We start with with defining the model for a generic $AR(n)$ sequence. The auto-regressive order n is passed to the model by means of the `order` variable.

$$
y(t) = \left[ \sum_{i=1}^{n} \alpha_i y(t-i) \right] + \eta(t) \qquad \eta \sim \mathcal{N}(0, \sigma^2)
$$

For any order $n ≥ 1$ the following model will correctly implement an $AR(n)$ process, which allows us to automatically test multiple AR processes easily and at once.

```{r}
# AR(n) string
modelAR_string <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrapping the AR process
  mu[1:order]     <- y[1:order]
  yp_in[1:order]  <- y[1:order]
  
  # 2. Recursive step of the AR process
  for (t in (order+1):N_train) {
    mu[t]     <- inprod(alpha_rev, y[(t-order):(t-1)]) + m0
    y[t]      ~  dnorm(mu[t], tau)
    yp_in[t]  ~  dnorm(mu[t], tau)
  }
  
  for (t in (train_metrics_skip+1):N_train) {
    loglik[t-train_metrics_skip] <- logdensity.norm(y[t], mu[t], tau)
  }
  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################
  
  # Init helper array z
  z[1:order] <- y[(N_train-order+1):N_train]
  
  for (t in (order+1):(order+N_test)) {
    mu_z[t] <- inprod(alpha_rev, z[(t-order):(t-1)]) + m0
    z[t]    ~  dnorm(mu_z[t], tau)
  }
  
  yp_out <- z[(order+1):(order+N_test)]
  
  ############################################################
  ############ ONE STEP AHEAD PREDICTION #####################
  ############################################################
  
  for (t in 1:N_test) {
    mu_onestep[t]     <- inprod(alpha_rev, y[(N_train+t-order):(N_train+t-1)]) + m0
    yp_onestep_out[t]  ~ dnorm(mu_onestep[t], tau)
  }
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0  ~ dnorm(0.0, 1.0E-4)
  tau ~ dgamma(0.1, 10)
  for (i in 1:order) {
    alpha[i] ~ dunif(-1.5, 1.5)
    alpha_rev[order - i + 1] <- alpha[i]
  }
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

We now define an $MA(1)$ process.

$$
y(t) = \alpha \eta(t-1) + \eta(t) \qquad \eta \sim \mathcal{N}(0, \sigma^2)
$$

```{r}
modelMA_string_legacy <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrap the MA(1) sequence
  e[1]      <- 0    # ALTERNATIVE: e[1] ~ dnorm(0, tau)
  yp_in[1]  <- y[1]
  
  # 2. Recursive step of the MA process
  for (t in 2:N_train) {
    mu[t]     <- m0 + beta * e[t-1]
    y[t]      ~  dnorm(mu[t], tau)
    yp_in[t]  ~  dnorm(mu[t], tau)
    
    # Extracting actual WN value
    e[t]  <- y[t] - mu[t]
  }

  for (t in (train_metrics_skip+1):N_train) {
    loglik[t-train_metrics_skip] <- logdensity.norm(y[t], mu[t], tau)
  }
  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################

  # 1. Bootstrappingthe out-of-sample predictor
  e_pred[1] ~  dnorm(0, tau)
  mu_out[1] <- m0 + beta * e[N_train]
  yp_out[1] ~  dnorm(mu_out[1], tau)
  
  # 2. Recursive step of the out-of-sample predictor
  for (t in 2:N_test) {
    mu_out[t] <- m0 + beta * e_pred[t-1]
    yp_out[t] ~  dnorm(mu_out[t], tau)
    e_pred[t] ~  dnorm(0, tau)
  }
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  tau   ~ dgamma(0.01, 0.01)
  beta  ~ dunif(-1, 1)
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

```{r}
modelMA_string_predictor_legacy <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrap the MA(1) sequence
  e[1]      <- 0    # ALTERNATIVE: e[1] ~ dnorm(0, tau)
  yp_in[1]  <- y[1]
  
  # 2. Recursive step of the MA process
  for (t in 2:N_train) {
    mu[t]     <- m0 + beta * e[t-1]
    y[t]      ~  dnorm(mu[t], tau)
    yp_in[t]  ~  dnorm(m0 + beta * (y[t-1] - yp_in[t-1]), tau)
    
    # Extracting actual WN value
    e[t] <- y[t] - mu[t]
  }

  for (t in (train_metrics_skip+1):N_train) {
    loglik[t-train_metrics_skip] <- logdensity.norm(y[t], mu[t], tau)
  }
  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################

  # ALWAYS 1-STEP-AHEAD METHOD
  # # 1. Bootstrappingthe out-of-sample predictor
  # e_pred[1] ~  dnorm(0, tau)
  # mu_out[1] <- m0 + beta * e[N_train]
  # yp_out[1] ~  dnorm(mu_out[1], tau)
  # 
  # # 2. Recursive step of the out-of-sample predictor
  # for (t in 2:N_test) {
  #   mu_out[t] <- m0 + beta * e_pred[t-1]
  #   yp_out[t] ~  dnorm(m0 + beta * (yp_out[t-1] - mu_out[t-1]), tau)
  #   e_pred[t] ~  dnorm(0, tau)
  # }
  
  # INCREMENTAL K-STEP AHEAD METHOD
  # 1. Bootstrappingthe out-of-sample predictor
  yp_out[1] ~  dnorm(m0 + beta * e[N_train], tau)

  # 2. Recursive step of the out-of-sample predictor
  for (t in 2:N_test) {
    yp_out[t] ~  dnorm(m0, tau / (1 + beta^2))
  }
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  tau   ~ dgamma(0.01, 0.01)
  beta  ~ dunif(-1, 1)
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

```{r}
modelMA_string <- "
model {

  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################

  e[1:order] = rep(0, order)
  yp_in[1:order]  <- y[1:order]
  
  # 2. Recursive step of the MA process
  for (t in (order+1):N_train) {
    mu[t]    <- m0 + inprod(beta_rev, e[(t-order):(t-1)])
    y[t]      ~ dnorm(mu[t], tau)
    yp_in[t]  ~ dnorm(mu[t], tau)
    e[t]     <- y[t] - mu[t]
  }

  for (t in (train_metrics_skip+1):N_train) {
    loglik[t-train_metrics_skip] <- logdensity.norm(y[t], mu[t], tau)
  }
  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################

  for (t in 1:N_test) {
    e[N_train+t] ~ dnorm(0, tau)
    mu_out[t]   <- m0 + inprod(beta_rev, e[(N_train+t-order):(N_train+t-1)])
    yp_out[t]    ~ dnorm(mu_out[t], tau)
  }
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  tau   ~ dgamma(0.01, 0.01)

  for (i in 1:order){
    beta[i]      ~ dunif(-1.1, 1.1)
    beta_rev[i] <- beta[order-i+1]
  }
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

The following model will merge the two previous models defining an $ARMA(1, 1)$ model.

$$
y(t) = \beta y(t-1) + \alpha \eta(t-1) + \eta(t) \qquad \eta \sim \mathcal{N}(0, \sigma^2).
$$

The choice for the auto-regressive part is a result of the analysis of the performance of $AR(n)$ models with $n > 1$ which seemed to indicate that a shallower memory was better.

```{r}
modelARMA_string_legacy <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrap the ARMA sequence
  e[1]      <- 0    # ALTERNATIVE: e[1] ~ dnorm(0, tau)
  yp_in[1]  <- y[1]

  # 2. Recursive step of the ARMA process
  for (t in 2:N_train) {
    mu[t]     <- m0 + beta * y[t-1] + alpha * e[t-1]
    y[t]      ~  dnorm(mu[t], tau)
    yp_in[t]  ~  dnorm(mu[t], tau)
    
    # Extracting actual WN value
    e[t]  <- y[t] - mu[t]
  }

  for (t in (train_metrics_skip+1):N_train) {
    loglik[t-train_metrics_skip] <- logdensity.norm(y[t], mu[t], tau)
  }

  ############################################################
  ######################## PREDICTION ########################
  ############################################################
  
  # 1. Bootstrappingthe out-of-sample predictor
  e_pred[1] ~  dnorm(0, tau)
  mu_out[1] <- m0 + beta * y[N_train] + alpha * e[N_train]
  yp_out[1] ~  dnorm(mu_out[1], tau)
  
  # 2. Recursive step of the out-of-sample predictor
  for (t in 2:N_test) {
    mu_out[t] <- m0 + beta * yp_out[t-1] + alpha * e_pred[t-1]
    yp_out[t] ~  dnorm(mu_out[t], tau)
    e_pred[t] ~  dnorm(0, tau)
  }
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  beta  ~ dunif(-1, 1)
  alpha ~ dunif(-1, 1)
  tau   ~ dgamma(0.01, 0.01)
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}
"
```

```{r}
modelARMA_string <- "
model {

  # order = max(order_ar, order_ma)
  
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################

  e[1:order]     <- rep(0, order)
  mu[1:order]    <- y[1:order]
  yp_in[1:order] <- y[1:order]
  
  # 2. Recursive step of the MA process
  for (t in (order+1):N_train) {
    mu_ar[t] <- inprod(alpha_rev, y[(t-order):(t-1)])
    mu_ma[t] <- inprod(beta_rev, e[(t-order):(t-1)])
    mu[t]    <- m0 + mu_ar[t] + mu_ma[t]
    y[t]      ~ dnorm(mu[t], tau)
    yp_in[t]  ~ dnorm(mu[t], tau)
    e[t]     <- y[t] - mu[t]
  }

  for (t in (train_metrics_skip+1):N_train) {
    loglik[t-train_metrics_skip] <- logdensity.norm(y[t], mu[t], tau)
  }
  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################

  z[1:order]   <- y[(N_train-order+1):N_train]
  e_z[1:order] <- e[(N_train-order+1):N_train]

  for (t in (order+1):(order+N_test)) {
    e_z[t]        ~ dnorm(0, tau)
    mu_ar_out[t] <- inprod(alpha_rev, z[(t-order):(t-1)])
    mu_ma_out[t] <- inprod(beta_rev, e_z[(t-order):(t-1)])
    mu_z[t]      <- m0 + mu_ar_out[t] + mu_ma_out[t]
    z[t]          ~ dnorm(mu_z[t], tau)
  }

  yp_out <- z[(order+1):(order+N_test)]
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  tau   ~ dgamma(0.01, 0.01)

  # for (i in 1:order) {
  #   alpha[i]      ~ dunif(-1.1, 1.1)
  #   alpha_rev[i] <- alpha[order-i+1]
  #   
  #   # beta[i]      ~ dunif(-1.1, 1.1)
  #   # beta_rev[i] <- beta[order-i+1]
  # }
  
  for (i in 1:order_ar) {
    alpha[i] ~ dunif(-1.1, 1.1)
  }
  for (i in (order_ar+1):order) {
    # beta[i]      ~ dnorm(0, 10000)
    alpha[i]      <- 0
  }
  
  for (i in 1:order) {
    alpha_rev[i] <- alpha[order-i+1]
  }

  for (i in 1:order_ma) {
    beta[i] ~ dunif(-1.1, 1.1)
  }
  for (i in (order_ma+1):order) {
    # beta[i]      ~ dnorm(0, 10000)
    beta[i]      <- 0
  }
  
  for (i in 1:order) {
    beta_rev[i] <- beta[order-i+1]
  }
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
  
}
"
```

## GDP time series

We will, at first, start off with some $AR(n)$ models, with increasing values of $n$. We will use the obtained results ot decide whether or not the selected model may apply to the problem at hand, and we will also evaluate which regressive order is best.

```{r}
# Metrics Dataframe
metrics_df_GDP <- data.frame(
  Model = character(),
  DIC   = numeric(),  
  WAIC  = numeric(), 
  BIC   = numeric(),
  MSE   = numeric(),  
  MAE   = numeric(), 
  MAPE  = numeric(),
  SMAPE = numeric(),  
  MASE  = numeric(),
  stringsAsFactors = FALSE
)

metrics_df_CPI <- data.frame(
  Model = character(),
  DIC   = numeric(),  
  WAIC  = numeric(), 
  BIC   = numeric(),
  MSE   = numeric(),  
  MAE   = numeric(), 
  MAPE  = numeric(),
  SMAPE = numeric(),  
  MASE  = numeric(),
  stringsAsFactors = FALSE
)

```

```{r}
to_save <- c(
  "alpha",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "yp_onestep_out",
  "train_metrics_skip",
  "loglik"
)

# Creating placeholder to store outputs
o <- list()

# Trying our AR(n) from n=1 to max_order
max_order <- 9
for (order in 1:max_order) {
  data_list <- list(
    "y"=series_GDP,
    "order"=order,
    "N_train"=N_train,
    "N_test"=N_test,
    "train_metrics_skip"=50
  )
  
  # Perform simulations and visualise output
  output <- fit_JAGS(
    modelAR_string,
    data_list,
    to_save
  )
  visualise(x_axis, series_GDP, output, title=glue("GDP series, AR({order})"))
  
  if (order == 1) {
    o = output
  }
  
  mets <- compute_metrics(output, series_GDP, N_train, k=order+2)
  metrics_df_GDP <- rbind(
    metrics_df_GDP,
    data.frame(
      Model = paste0("AR(",order,")"),
      DIC   = mets$DIC,
      WAIC  = mets$WAIC,
      BIC   = mets$BIC,
      MSE   = mets$MSE,
      MAE   = mets$MAE,
      MAPE  = mets$MAPE,
      SMAPE = mets$SMAPE,
      MASE  = mets$MASE,
      stringsAsFactors = FALSE
    )
  )
}
```

```{r}
# Extract only AR(1)
output <- o

# Inspecting results
parameters <- c("alpha", "m0")
mcmc_trace(output$samples, pars=parameters)
mcmc_areas(output$samples, pars=parameters, prob=0.95)

print(output)
```

We will now also fit an MA(1) model to evaluate whether it is a better model.

```{r}
to_save <- c(
  "beta",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "train_metrics_skip",
  "loglik"
)

# Creating placeholder to store outputs
o <- list()

# Trying our AR(n) from n=1 to max_order
max_order <- 5
for (order in 1:max_order) {
  data_list <- list(
    "y"=series_GDP,
    "order"=order,
    "N_train"=N_train,
    "N_test"=N_test,
    "train_metrics_skip"=50
  )
  
  # Perform simulations and visualise output
  output <- fit_JAGS(
    modelMA_string,
    data_list,
    to_save
  )
  visualise(x_axis, series_GDP, output, title=glue("GDP series, MA({order})"))
  
  if (order == 1) {
    o = output
  }
  
  mets <- compute_metrics(output, series_GDP, N_train, k=order+2)
  metrics_df_GDP <- rbind(
    metrics_df_GDP,
    data.frame(
      Model = paste0("MA(",order,")"),
      DIC   = mets$DIC,
      WAIC  = mets$WAIC,
      BIC   = mets$BIC,
      MSE   = mets$MSE,
      MAE   = mets$MAE,
      MAPE  = mets$MAPE,
      SMAPE = mets$SMAPE,
      MASE  = mets$MASE,
      stringsAsFactors = FALSE
    )
  )
}
```

```{r}
# Inspecting results
parameters <- c("beta", "m0")
mcmc_trace(o$samples, pars=parameters)
mcmc_areas(o$samples, pars=parameters, prob=0.95)

print(output)
```

```{r}
to_save <- c(
  "alpha",
  "beta",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "train_metrics_skip",
  "loglik"
)

# Creating placeholder to store outputs
o <- list()

# Trying our AR(n) from n=1 to max_order
max_order_ar <- 5
max_order_ma <- 2
for (order_ma in 1:max_order_ma) {
  for (order_ar in 1:max_order_ar) {
    data_list <- list(
      "y"=series_GDP,
      "N_train"=N_train,
      "N_test"=N_test,
      "order"=max(order_ar, order_ma),
      "order_ar"=order_ar,
      "order_ma"=order_ma,
      "train_metrics_skip"=50
    )
    
    # Perform simulations and visualise output
    output <- fit_JAGS(
      modelMA_string,
      data_list,
      to_save
    )
    visualise(x_axis, series_GDP, output, title=glue("GDP series, ARMA({order_ar}, {order_ma})"))
    
    if (order == 1) {
      o = output
    }
    
    mets <- compute_metrics(output, series_GDP, N_train, k=order+2)
    metrics_df_GDP <- rbind(
      metrics_df_GDP,
      data.frame(
        Model = glue("ARMA({order_ar}, {order_ma})"),
        DIC   = mets$DIC,
        WAIC  = mets$WAIC,
        BIC   = mets$BIC,
        MSE   = mets$MSE,
        MAE   = mets$MAE,
        MAPE  = mets$MAPE,
        SMAPE = mets$SMAPE,
        MASE  = mets$MASE,
        stringsAsFactors = FALSE
      )
    )
  }
}
```

## VAR

Finally, we will to fit generic order VAR(n) models

```{r}
modelVAR_string = "
model {

  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  mu[1:2,1:order] = y[,1:order]
  yp_in[1:2,1:order] = y[,1:order]
  
  for (t in (order+1):N_train) {
    mu_tmp[1:2, t, 1] <- m0
    for (j in 1:order) {
      mu_tmp[1:2,t,j+1] <- mu_tmp[1:2, t, j] + A[,,j] %*% y[,(t-j)]
    }
    mu[1:2,t]   <- mu_tmp[1:2,t,order+1]
    y[1:2,t]     ~ dmnorm(mu[1:2,t], tau)
    yp_in[1:2,t] ~ dmnorm(mu[1:2,t], tau)

    # Compute univariate log-likelihood
    # loglik_gdp[t]  <- logdensity.norm(y[1,t], mu[1,t], 1 / sigma2[1,1])
    # loglik_infl[t] <- logdensity.norm(y[2,t], mu[2,t], 1 / sigma2[2,2])
  }
  
  # for (t in 1:order) {
  #   loglik_gdp[t]  <- logdensity.norm(y[1,t], mu[1,t], 1 / sigma2[1,1])
  #   loglik_infl[t] <- logdensity.norm(y[2,t], mu[2,t], 1 / sigma2[2,2])
  # }

  for (t in (train_metrics_skip+1):N_train) {
    loglik_gdp[t-train_metrics_skip]  <- logdensity.norm(y[1,t], mu[1,t], 1 / sigma2[1,1])
    loglik_infl[t-train_metrics_skip] <- logdensity.norm(y[2,t], mu[2,t], 1 / sigma2[2,2])
  }


  ############################################################
  ######################## PREDICTION ########################
  ############################################################
  
  z[1:2,1:order] = y[,(N_train-order+1):N_train]
  
  for (t in (order+1):(order+N_test)) {
    mu_z[1:2, t, 1] <- m0
    for (j in 1:order) {
      mu_z[1:2,t,j+1] <- mu_z[1:2, t, j] + A[,,j] %*% z[,(t-j)]
    }
    z[1:2,t] ~ dmnorm(mu_z[1:2,t,order+1], tau)
  }
  
  yp_out[1:2,1:N_test] = z[1:2,(order+1):(order+N_test)]
  
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################

  for (i in 1:2) {
    for (j in 1:2) {
      for (o in 1:order) {
        # A[i, j, o] ~ dunif(-1.2, 1.2)
        A[i, j, o] ~ dnorm(0, 10)
      }
    }
  }
  
  tau     ~ dwish(R, 3) # dwish(ScaleMatrix... Identity should be ok, df)
  m_gdp   ~ dnorm(0.0, 1.0E-4)
  m_infl  ~ dnorm(0.0, 1.0E-4)
  m0     <- c(m_gdp, m_infl)
  
  sigma2 <- inverse(tau)
  
  
  ############################################################
  ######################## EXTRACTION ########################
  ############################################################

  # Expose them for output in R
  loglik_gdp_out   <- loglik_gdp
  loglik_infl_out  <- loglik_infl

  yp_in_gdp   <- yp_in[1,]
  yp_in_infl  <- yp_in[2,]
  yp_out_gdp  <- yp_out[1,]
  yp_out_infl <- yp_out[2,]

}"
```

```{r}
visualise_VAR = function(
  x_axis,               # Values to be displayed on the x-axis of the plots
  original_series_gdp,  # Original GDP time series with all samples
  original_series_infl, # Original INFLATION time series with all samples
  output,               # Output from VAR JAGS, with in-sample and out-of-sample predictions
  title="",             # Title to display on the graphs
  keep=5                # Training sampled to keep in the test graph
) {
  
  output_gdp = list()
  output_gdp$mean$yp_in   = output$mean$yp_in_gdp
  output_gdp$q2.5$yp_in   = output$q2.5$yp_in_gdp
  output_gdp$q97.5$yp_in  = output$q97.5$yp_in_gdp
  output_gdp$mean$yp_out  = output$mean$yp_out_gdp
  output_gdp$q2.5$yp_out  = output$q2.5$yp_out_gdp
  output_gdp$q97.5$yp_out = output$q97.5$yp_out_gdp
  output_gdp$q25$yp_out   = output$q25$yp_out_gdp
  output_gdp$q75$yp_out   = output$q75$yp_out_gdp
  output_gdp$mean$m0      = output$mean$m_gdp
  
  output_infl = list()
  output_infl$mean$yp_in   = output$mean$yp_in_infl
  output_infl$q2.5$yp_in   = output$q2.5$yp_in_infl
  output_infl$q97.5$yp_in  = output$q97.5$yp_in_infl
  output_infl$mean$yp_out  = output$mean$yp_out_infl
  output_infl$q2.5$yp_out  = output$q2.5$yp_out_infl
  output_infl$q97.5$yp_out = output$q97.5$yp_out_infl
  output_infl$q25$yp_out   = output$q25$yp_out_infl
  output_infl$q75$yp_out   = output$q75$yp_out_infl
  output_infl$mean$m0      = output$mean$m_infl
  
  visualise(x_axis, original_series_gdp, output_gdp, title, keep)
  visualise(x_axis, original_series_infl, output_infl, title, keep)
}
```

```{r}
R = matrix(c(1, 0, 0, 1), nrow=2, byrow=TRUE)
y = matrix(c(series_GDP, series_CPI), nrow=2, byrow=TRUE)

order=10

to_save <- c(
  "A",
  "m_gdp",
  "m_infl",
  "sigma2",
  "yp_in_gdp",
  "yp_in_infl",
  "yp_out_gdp",
  "yp_out_infl",
  "train_metrics_skip",
  "loglik_gdp",   # marginal log-likelihood for GDP
  "loglik_infl"   # marginal log-likelihood for CPI
)

data_list <- list(
  "y"=y,
  "R"=R,
  "order"=order,
  "N_train" = N_train,
  "N_test" = N_test,
  "train_metrics_skip"=50
)

jags_output_VAR = fit_JAGS(
  modelVAR_string,
  data_list,
  to_save,
  n_iter=6000,
  n_adapt=1000,
  n_chain=1,
  n_burnin=2000)

visualise_VAR(
  x_axis,
  series_GDP,
  series_CPI,
  jags_output_VAR,
  title=glue("VAR({order})"),
  keep=5
)


# Wrapper for GDP-only metrics
output_gdp <- list(
  mean      = list(yp_out = jags_output_VAR$mean$yp_out_gdp, train_metrics_skip=jags_output_VAR$mean$train_metrics_skip),
  sims.list = list(loglik  = jags_output_VAR$sims.list$loglik_gdp)
)

# Wrapper for CPI-only metrics
output_infl <- list(
  mean      = list(yp_out = jags_output_VAR$mean$yp_out_infl, train_metrics_skip=jags_output_VAR$mean$train_metrics_skip),
  sims.list = list(loglik  = jags_output_VAR$sims.list$loglik_infl)
)


# Compute univariate metrics for GDP and CPI
mets_VAR_gdp  <- compute_metrics(output_gdp,  series_GDP, N_train, k = 4*order + 2 + 3)
mets_VAR_infl <- compute_metrics(output_infl, series_CPI, N_train, k = 4*order + 2 + 3)

metrics_df_GDP <- rbind(
  metrics_df_GDP,
  data.frame(
    Model = paste0("VAR_univar_GDP(", order, ")"),
    DIC   = mets_VAR_gdp$DIC,
    WAIC  = mets_VAR_gdp$WAIC,
    BIC   = mets_VAR_gdp$BIC,
    MSE   = mets_VAR_gdp$MSE,
    MAE   = mets_VAR_gdp$MAE,
    MAPE  = mets_VAR_gdp$MAPE,
    SMAPE = mets_VAR_gdp$SMAPE,
    MASE  = mets_VAR_gdp$MASE,
    stringsAsFactors = FALSE
  )
)

metrics_df_CPI <- rbind(
  metrics_df_CPI,
  data.frame(
    Model = paste0("VAR_univar_CPI(", order, ")"),
    DIC   = mets_VAR_infl$DIC,
    WAIC  = mets_VAR_infl$WAIC,
    BIC   = mets_VAR_infl$BIC,
    MSE   = mets_VAR_infl$MSE,
    MAE   = mets_VAR_infl$MAE,
    MAPE  = mets_VAR_infl$MAPE,
    SMAPE = mets_VAR_infl$SMAPE,
    MASE  = mets_VAR_infl$MASE,
    stringsAsFactors = FALSE
  )
)
```

## Metrics

```{r}
# GDP table
kable(
  metrics_df_GDP,
  caption = "Univariate model metrics — GDP series",
  digits  = c(1,1,1,1,3,3,1,1,3),
  format  = "html",
  table.attr = 'cellpadding="10" cellspacing="0"'
)

# CPI table
kable(
  metrics_df_CPI,
  caption = "Univariate model metrics — CPI series",
  digits  = c(1,1,1,1,3,3,1,1,3),
  format  = "html",
  table.attr = 'cellpadding="10" cellspacing="0"'
)
```

PAY ATTENTION:

The MAPE exceeds 100% because the series (quarterly or monthly GDP growth rates) has actual values (denominators) that are very small or close to zero in some periods, which amplifies the percentage.

## AR order selection

```{r}
# Metrics Dataframe
metrics_df_GDP_AR <- data.frame(
  Model = character(),
  DIC   = numeric(),
  WAIC  = numeric(),
  BIC   = numeric(),
  MSE   = numeric(),
  MAE   = numeric(),
  MAPE  = numeric(),
  SMAPE = numeric(),
  MASE  = numeric(),
  stringsAsFactors = FALSE
)

to_save <- c(
  "alpha",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "yp_onestep_out",
  "train_metrics_skip",
  "loglik"
)

# Creating placeholder to store outputs
o <- list()

# Trying our AR(n) from n=1 to max_order
max_order <- 5
for (order in 1:max_order) {
  
  data_list <- list(
    "y"=series_GDP,
    "order"=order,
    "N_train"=N_train,
    "N_test"=N_test,
    "train_metrics_skip"=50
  )
  
  # Perform simulations and visualise output
  output <- fit_JAGS(
    modelAR_string,
    data_list,
    to_save
  )
  visualise(x_axis, series_GDP, output, title=glue("GDP series, AR({order})"))
  
  if (order == 1) {
    o = output
  }
  
  mets <- compute_metrics(output, series_GDP, N_train, k=order+2)
  metrics_df_GDP_AR <- rbind(
    metrics_df_GDP_AR,
    data.frame(
      Model = paste0("AR(",order,")"),
      DIC   = mets$DIC,
      WAIC  = mets$WAIC,
      BIC   = mets$BIC,
      MSE   = mets$MSE,
      MAE   = mets$MAE,
      MAPE  = mets$MAPE,
      SMAPE = mets$SMAPE,
      MASE  = mets$MASE,
      stringsAsFactors = FALSE
    )
  )
}
```

```{r}

# saveRDS(metrics_df_GDP_AR, file = "metrics_df_GDP_AR.rds")

```

```{r}

plot(1:max_order, metrics_df_GDP_AR[1:max_order,2], type="l", main="DIC")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,3], type="l", main="WAIC")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,4], type="l", main="BIC")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,5], type="l", main="MSE")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,6], type="l", main="MAE")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,7], type="l", main="MAPE")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,8], type="l", main="SMAPE")
plot(1:max_order, metrics_df_GDP_AR[1:max_order,9], type="l", main="MASE")

```

## Anomaly detection

In this last section we will examine anomalies and outliers which are present in our two time series. So far, we performed model selection and predictions of future values for the series, however we could clearly see that both the training and test set presented samples which deviated quite heavily from their neighbouring values. This can be cause of disturbances during the learning phase and also a reduced accuracu during forecasting.

The objective of this section is to detect said outliers and visualise them so as to clearly show which samples may be problematic during fitting and out-of-sample prediction.

In order to do this, for each timestamp $t$, we consider the incremental values of our series, each of which is to be treated as an independent random variable:

$$
\Delta_t := y_{t} - y_{t-1} \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma_t^2)
$$

In order to be able to perform anomaly detection, we must ensure that the sequence of incremental values $\Delta_t$ can be approximated to a random walk. This can be verified by trying to fit an $AR(1)$ process to the initial sequence of values $y_t$ and checking whether the value $|\alpha|$ is close to 1:

```{r}
# Data lists for both delta-series
data_list_GDP <- list(
  "y"=series_GDP,
  "train_metrics_skip"=50,
  "order"=1,
  "N_train"=N_tot,
  "N_test"=1          # No need to perform forecasting
)
data_list_CPI <- list(
  "y"=series_CPI,
  "train_metrics_skip"=50,
  "order"=1,
  "N_train"=N_tot,
  "N_test"=1          # No need to perform forecasting
)

# Only need to inspect value of alpha for both series
to_save <- c(
  "alpha"
)

# Fit AR(1) models
output_GDP <- fit_JAGS(
  modelAR_string,
  data_list_GDP,
  to_save
)
output_CPI <- fit_JAGS(
  modelAR_string,
  data_list_CPI,
  to_save
)

# Inspecting values of alpha
print(glue("Alpha value for GDP has posterior mean {output_GDP$mean$alpha} with standard deviation of {output_GDP$sd$alpha}"))
print(glue("Alpha value for CPI has posterior mean {output_CPI$mean$alpha} with standard deviation of {output_CPI$sd$alpha}"))
```

Now that we know that the two series can be approximated reasonably well to random walks ($\alpha_{GDP} = 0.86$ and $\alpha_{CPI} = 0.938$) we proceede with our anomaly detection.

Notice that the random variable $\Delta_t$ has a fixed mean value $\mu$, but a time dependant variance $\sigma_t^2$, which we model as:

$$
\sigma_t^{-2} = \tau_t = \beta_1 + \beta_2 \delta_t \qquad \delta_t \sim Ber(p)
$$

We can, therefore, define the following JAGS model-string which will fit the individual values of $\delta_t$.

```{r}
anomaly_detection_string <- "
model {

  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################

  for(t in 1:N_tot) {
    delta[t]  ~  dnorm(mu, tau[t])
    tau[t]    <- beta[1] + beta[2] * d[t]
  }
  
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  # Beta coefficients, Bernoulli probability and mean
  beta[1] ~ dexp(0.1)
  beta[2] ~ dexp(0.1)
  p       ~ dbeta(1, 1)
  mu      ~ dnorm(0, 0.01)
  
  # Delta value
  for(t in 1:N_tot){
    d[t] ~ dbern(p)
  }
}"
```

```{r}
# Obtaining series of differences
delta_GDP <- series_GDP[2:N_tot] - series_GDP[1:(N_tot-1)]
delta_CPI <- series_CPI[2:N_tot] - series_CPI[1:(N_tot-1)]

# Setting up parameters for JAGS
data_list_GDP <- list(
  "delta"=delta_GDP,
  "N_tot"=N_tot-1
)
data_list_CPI <- list(
  "delta"=delta_CPI,
  "N_tot"=N_tot-1
)
to_save <- c(
  "d",
  "mu",
  "tau"
)

# Perform simulations
anomaly_output_GDP <- fit_JAGS(
  anomaly_detection_string,
  data_list_GDP,
  to_save,
  n_iter=10000
)
anomaly_output_CPI <- fit_JAGS(
  anomaly_detection_string,
  data_list_CPI,
  to_save,
  n_iter=10000
)
```

Once obtained both $\{\delta_t\}_{t=1...N}$ sequences we can classify their values according to their expected values. Concretely, we classify a timestamp $t$ such that $\mathbb{E}[\delta_t] < 0.2$ as *"high variance"* (where strong oscillations may occur) and timestamps $t$ with $0.2 < \mathbb{E}[\delta_t] < 0.6$ as samples with *"medium variance"* (where we still might have an anomaly, but of weaker nature). All remaining samples can be considered stable and only subject to natural stochastic oscillations.

```{r}
# Classifying anomalies for both series
anomalies_GDP <- classify_anomalies(anomaly_output_GDP, N_tot, title="GDP")
anomalies_CPI <- classify_anomalies(anomaly_output_CPI, N_tot, title="CPI")

# Displaying graphically the classification
delta_GDP <- anomaly_output_GDP$mean$d
delta_CPI <- anomaly_output_CPI$mean$d

display_classification(
  x_axis,
  delta_GDP,
  anomalies_GDP$high_var_indices,
  anomalies_GDP$med_var_indices,
  anomalies_GDP$low_var_indices,
  title="GDP"
)
display_classification(
  x_axis,
  delta_CPI,
  anomalies_CPI$high_var_indices,
  anomalies_CPI$med_var_indices,
  anomalies_CPI$low_var_indices,
  title="CPI"
)
```

Finally, we can plot both our time series displaying the points in time where high variances were observed: red vertical lines display timestamps where strong anomalies were detected, while orange dotted lines show medium anomalies.

```{r}
# Zoom value: we only plot starting from this point on so as not to clog up the graph
start <- 150

visualise_anomalies(
  x_axis,
  series_GDP,
  anomalies_GDP$high_var_indices,
  anomalies_GDP$med_var_indices,
  title="GDP",
  start=start
)
visualise_anomalies(
  x_axis,
  series_CPI,
  anomalies_CPI$high_var_indices,
  anomalies_CPI$med_var_indices,
  title="CPI",
  start=start
)
```
