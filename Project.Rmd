---
title: "Project"
output: html_document
date: "2025-06-20"
---

```{r}
rm(list=ls())
library(rjags)
library(bayesplot)
library(jagsUI)
library(glue)
library(loo)
library(knitr)
```

## Bayesian Learning & Monte-Carlo Simulation final project

Setting up environment, importing the dataset and visualising a graph of the two time series we will have to fit.

```{r}
rm(list=ls())

# New Family Houses Sold: United States
# Source: https://fred.stlouisfed.org/series/HSN1F
data      <- read.csv("gdp_inflation.csv",header=T)
data$DATE <- as.Date(data$DATE)

# Plotting time series for visualisation purposes
plot(
  data$DATE[1:305],
  data$GDP_PC1[1:305],
  type="l",
  xlab="",
  ylab="GDP",
  main="GDP + CPI"
)
lines(
  data$DATE[1:305],
  data$CPIAUCSL_PC1[1:305],
  type="l",
  col="red"
)
```

At this point, we setup our environment by converting the values in both time series into numericals, and then splitting them into training and testing. Finally, we define some useful variables which we will be using through out our script.

```{r}
# Dataset extraction
x_axis      <- data$DATE[1:305]
series_GDP  <- as.numeric(data$GDP_PC1[1:305])
series_CPI  <- as.numeric(data$CPIAUCSL_PC1[1:305])

# Parameter setup (RMK: we assume both time series of same length)
TRAIN_PERC  <- 0.15
N_tot       <- length(series_GDP)
N_test      <- floor(TRAIN_PERC * N_tot)
N_train     <- N_tot - N_test
```

To further inspect our data we can also plot the respective auto-correlation functions:

```{r}
acf(series_GDP, lag.max=100)
acf(series_CPI, lag.max=100)
```

We now define a few utility functions which will be useful later. The first one automatises the interactions with JAGS, the second one will draw plots of both the whole time-series and its out-of-sample portion only.

```{r}
fit_JAGS <- function(
  model_string,   # JAGS model string
  data_list,      # List of input values for JAGS
  to_save,        # Vector with parameter names which should be saved
  n_iter=5000,    # Number of iterations. Should be greater than n_burnin
  n_adapt=1000,   # JAGS n.adapt internal parameter
  n_chain=1,      # Number of chains to run
  n_burnin=1000,  # Burn-in value
  n_thin=1        # Thinning value
) {
  
  output <- jags(
    model.file=textConnection(model_string),
    data=data_list,
    parameters.to.save=to_save,
    n.adapt=n_adapt,
    n.iter=n_iter,
    n.chains=n_chain,
    n.burnin=n_burnin,
    n.thin=n_thin
  )
  
  return(output)
}
```

```{r}
visualise <- function(
  x_axis,           # Values to be displayed on the x-axis of the plots
  original_series,  # Original time series with all samples
  output,           # Output from JAGS, with in-sample and out-of-sample predictions
  title="",         # Title to display on the graphs
  keep=5            # Training sampled to keep in the test graph
) {
  # Length of total series
  N <- length(x_axis)
  
  
  # Plotting entire graph (training-set + predictions)
  # In-sample predicted values with their 2.5%-97.5% quantiles
  yp_in_mean  <- output$mean$yp_in
  yp_in_q1    <- output$q2.5$yp_in
  yp_in_q2    <- output$q97.5$yp_in
  in_length   <- length(yp_in_mean)
  
  # Out-of-sample predicted values with their 2.5%-97.5% quantiles and 25%-75% quantiles
  yp_out_mean <- output$mean$yp_out
  yp_out_q1   <- output$q2.5$yp_out
  yp_out_q2   <- output$q97.5$yp_out
  yp_out_q3   <- output$q25$yp_out
  yp_out_q4   <- output$q75$yp_out
  
  # Removing potentially non-finite values from quantiles
  yp_in_q1[is.infinite(yp_in_q1)]   <- NA
  yp_in_q2[is.infinite(yp_in_q2)]   <- NA
  yp_out_q1[is.infinite(yp_out_q1)] <- NA
  yp_out_q2[is.infinite(yp_out_q2)] <- NA
  yp_out_q3[is.infinite(yp_out_q3)] <- NA
  yp_out_q4[is.infinite(yp_out_q4)] <- NA
  
  # Computing extreme values for first graph
  min_y <- min(original_series, yp_in_q1, yp_out_q1)
  max_y <- max(original_series, yp_in_q2, yp_out_q2)
  
  # Plotting REAL time series (as points only), both training set and test-set
  plot(
    x_axis,
    original_series,
    main=title, col="red",
    xlab="time", ylab="price",
    ylim=c(min_y, max_y)
  )
  lines(
    x_axis,
    original_series,
    col="red"
  )
  
  # Plotting a vertical line signalling end of training set and horizontal
  # Showing the computer mean
  abline(v=x_axis[in_length], col="magenta", lwd=1, lty=2)
  abline(h=output$mean$m0,    col="green",   lwd=1, lty=2)
  
  # Plotting in-sample predictions as well as 2.5%-97.5% quantiles
  points(x_axis[1:in_length], yp_in_mean, col="blue", pch="*")
  lines(x_axis[1:in_length],  yp_in_q1,   col="blue", lwd=1.5)
  lines(x_axis[1:in_length],  yp_in_q2,   col="blue", lwd=1.5)
  
  # Plotting out-of-sample predicted values as well as 2.5%-97.5%
  # quantiles and 25%-75% quantiles
  points(x_axis[(in_length+1):N], yp_out_mean, col="orange", pch="*")
  lines(x_axis[(in_length+1):N],  yp_out_q1,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q2,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q3,   col="orange", lwd=1.5, lty=2)
  lines(x_axis[(in_length+1):N],  yp_out_q4,   col="orange", lwd=1.5, lty=2)
  

  # Plotting onyl out-of-sample portion of graph (with last 5 training samples)
  # Re-computing extreme values for second graph
  min_y <- min(original_series[(in_length+1 - keep):N], yp_out_q1)
  max_y <- max(original_series[(in_length+1 - keep):N], yp_out_q2)
  
  # Plotting REAL time series (as points only), both training set and test-set
  plot(
    x_axis[(in_length+1 - keep):N],
    original_series[(in_length+1 - keep):N],
    main=title, col="red",
    xlab="time", ylab="price",
    ylim=c(min_y, max_y)
  )
  lines(
    x_axis[(in_length+1-keep):N],
    original_series[(in_length+1-keep):N],
    col="red"
  )
  
  # Plotting a vertical line signalling end of training set
  abline(v=x_axis[in_length+1], col="magenta", lwd=1, lty=2)
  abline(h=output$mean$m0,      col="green",   lwd=1, lty=2)
  
  # Plotting out-of-sample predicted values as well as 2.5%-97.5%
  # quantiles and 25%-75% quantiles
  points(x_axis[(in_length+1):N], yp_out_mean, col="orange", pch="*")
  lines(x_axis[(in_length+1):N],  yp_out_q1,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q2,   col="orange", lwd=1.5)
  lines(x_axis[(in_length+1):N],  yp_out_q3,   col="orange", lwd=1.5, lty=2)
  lines(x_axis[(in_length+1):N],  yp_out_q4,   col="orange", lwd=1.5, lty=2)
}
```

```{r}

# K is the number of effective params
compute_metrics <- function(output, series, N_train, k) {
  
  ############################################################
  ###################### IN-SAMPLE ###########################
  ############################################################
  # In-sample: DIC, WAIC, BIC
  DIC_val <- output$DIC
  
  ll_mat <- output$sims.list$loglik                       # draws × N_train
  waic_res <- suppressWarnings( waic(ll_mat) )
  WAIC_val <- waic_res$estimates["waic","Estimate"]
  
  
  #BIC computation
  #Sum of the log-likelihoods for each draw
  ll_sum <- rowSums(ll_mat)
  
  # maximum of the sum: estimate of log p(y|theta_hat), that is, log-lik_max
  lhat <- max(ll_sum)
  
  BIC_val <- -2 * lhat + k * log(N_train)
  
  # BIC Approx
  # BIC_val  <- DIC_val + k * (log(N_train) - 2) 
  
  ############################################################
  #################### OUT-OF-SAMPLE #########################
  ############################################################
  
  y_pred <- output$mean$yp_out
  y_true <- series[(N_train+1):length(series)]
  e <- y_pred - y_true
  
  # MSE
  MSE_val <- mean((e)^2)
  
  #MAE
  MAE_val <- mean(abs(e))
  MAPE_val <- mean(abs(e / y_true)) * 100
  
  #SMAPE
  SMAPE_val <- mean(2 * abs(e) / (abs(y_true) + abs(y_pred))) * 100
  
  #MASE
  scale <- mean(abs(diff(series[1:N_train])))
  MASE_val <- MAE_val / scale
  
 list(
    DIC   = round(DIC_val, 1),
    WAIC  = round(WAIC_val, 1),
    BIC   = round(BIC_val, 1),
    MSE   = round(MSE_val, 3),
    MAE   = round(MAE_val, 3),
    MAPE  = round(MAPE_val, 1),
    SMAPE = round(SMAPE_val, 1),
    MASE  = round(MASE_val, 3)
  )
}
```

compute_dic_marginal <- function(ll_mat) {
  dev      <- -2 * ll_mat
  dev_sum  <- rowSums(dev)
  Dbar     <- mean(dev_sum)
  Dhat     <- sum(-2 * colMeans(ll_mat))
  pD       <- Dbar - Dhat
  DIC      <- Dbar + pD
  return(DIC)
}

## JAGS model strings

At this point, we define all our JAGS model strings so as to be able to reuse them multiple times through out the script. We start with with defining the model for a generic $AR(n)$ sequence. The auto-regressive order n is passed to the model by means of the `order` variable.

$$
y(t) = \left[ \sum_{i=1}^{n} \alpha_i y(t-i) \right] + \eta(t) \qquad \eta \sim \mathcal{N}(0, \sigma^2)
$$

For any order $n ≥ 1$ the following model will correctly implement an $AR(n)$ process, which allows us to automatically test multiple AR processes easily and at once.

```{r}
# AR(n) string
modelAR_string <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrapping the AR process
  mu[1:order]     <- y[1:order]
  yp_in[1:order]  <- y[1:order]
  
  for (t in 1:order) {
    loglik[t] <- logdensity.norm(y[t], mu[t], tau)
  }
  
  # 2. Recursive step of the AR process
  for (t in (order+1):N_train) {
    mu[t]     <- inprod(alpha_rev, y[(t-order):(t-1)]) + m0
    y[t]      ~  dnorm(mu[t], tau)
    loglik[t] <- logdensity.norm(y[t], mu[t], tau)
    yp_in[t]  ~  dnorm(mu[t], tau)
  }
  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################
  
  # Init helper array z
  z[1:order] <- y[(N_train-order+1):N_train]
  
  for (t in (order+1):(order+N_test)) {
    mu_z[t] <- inprod(alpha_rev, z[(t-order):(t-1)]) + m0
    z[t]    ~  dnorm(mu_z[t], tau)
  }
  
  yp_out <- z[(order+1):(order+N_test)]
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0  ~ dnorm(0.0, 1.0E-4)
  tau ~ dgamma(0.1, 10)
  for (i in 1:order) {
    alpha[i] ~ dunif(-1.5, 1.5)
    alpha_rev[order - i + 1] <- alpha[i]
  }
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

We now define an $MA(1)$ process.

$$
y(t) = \alpha \eta(t-1) + \eta(t) \qquad \eta \sim \mathcal{N}(0, \sigma^2)
$$

```{r}
modelMA_string <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrap the MA(1) sequence
  e[1]      <- 0    # ALTERNATIVE: e[1] ~ dnorm(0, tau)
  yp_in[1]  <- y[1]
  
  loglik[1] <- logdensity.norm(y[1], m0, tau)
  
  # 2. Recursive step of the MA process
  for (t in 2:N_train) {
    mu[t]     <- m0 + beta * e[t-1]
    y[t]      ~  dnorm(mu[t], tau)
    loglik[t] <- logdensity.norm(y[t], mu[t], tau)
    yp_in[t]  ~  dnorm(mu[t], tau)
    
    # Extracting actual WN value
    e[t]  <- y[t] - mu[t]
  }

  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################

  # 1. Bootstrappingthe out-of-sample predictor
  e_pred[1] ~  dnorm(0, tau)
  mu_out[1] <- m0 + beta * e[N_train]
  yp_out[1] ~  dnorm(mu_out[1], tau)
  
  # 2. Recursive step of the out-of-sample predictor
  for (t in 2:N_test) {
    mu_out[t] <- m0 + beta * e_pred[t-1]
    yp_out[t] ~  dnorm(mu_out[t], tau)
    e_pred[t] ~  dnorm(0, tau)
  }
  
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  tau   ~ dgamma(0.01, 0.01)
  beta  ~ dunif(-1, 1)
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

```{r}
modelMA_string_predictor <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrap the MA(1) sequence
  e[1]      <- 0    # ALTERNATIVE: e[1] ~ dnorm(0, tau)
  yp_in[1]  <- y[1]
  loglik[1] <- logdensity.norm(y[1], m0, tau) 
  
  # 2. Recursive step of the MA process
  for (t in 2:N_train) {
    mu[t]     <- m0 + beta * e[t-1]
    y[t]      ~  dnorm(mu[t], tau)
    loglik[t] <- logdensity.norm(y[t], mu[t], tau)
    yp_in[t]  ~  dnorm(m0 + beta * (y[t-1] - yp_in[t-1]), tau)
    
    # Extracting actual WN value
    e[t] <- y[t] - mu[t]
  }

  
  ############################################################
  ######################## PREDICTION ########################
  ############################################################

  # ALWAYS 1-STEP-AHEAD METHOD
  # # 1. Bootstrappingthe out-of-sample predictor
  # e_pred[1] ~  dnorm(0, tau)
  # mu_out[1] <- m0 + beta * e[N_train]
  # yp_out[1] ~  dnorm(mu_out[1], tau)
  # 
  # # 2. Recursive step of the out-of-sample predictor
  # for (t in 2:N_test) {
  #   mu_out[t] <- m0 + beta * e_pred[t-1]
  #   yp_out[t] ~  dnorm(m0 + beta * (yp_out[t-1] - mu_out[t-1]), tau)
  #   e_pred[t] ~  dnorm(0, tau)
  # }
  
  # INCREMENTAL K-STEP AHEAD METHOD
  # 1. Bootstrappingthe out-of-sample predictor
  yp_out[1] ~  dnorm(m0 + beta * e[N_train], tau)

  # 2. Recursive step of the out-of-sample predictor
  for (t in 2:N_test) {
    yp_out[t] ~  dnorm(m0, tau / (1 + beta^2))
  }
  
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  tau   ~ dgamma(0.01, 0.01)
  beta  ~ dunif(-1, 1)
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}"
```

The following model will merge the two previous models defining an $ARMA(1, 1)$ model.

$$
y(t) = \beta y(t-1) + \alpha \eta(t-1) + \eta(t) \qquad \eta \sim \mathcal{N}(0, \sigma^2).
$$

The choice for the auto-regressive part is a result of the analysis of the performance of $AR(n)$ models with $n > 1$ which seemed to indicate that a shallower memory was better.

```{r}
modelARMA_string <- "
model {
  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  # 1. Bootstrap the ARMA sequence
  e[1]      <- 0    # ALTERNATIVE: e[1] ~ dnorm(0, tau)
  yp_in[1]  <- y[1]
  loglik[1] <- logdensity.norm(y[1], m0, tau)

  # 2. Recursive step of the ARMA process
  for (t in 2:N_train) {
    mu[t]     <- m0 + beta * y[t-1] + alpha * e[t-1]
    y[t]      ~  dnorm(mu[t], tau)
    loglik[t] <- logdensity.norm(y[t], mu[t], tau)
    yp_in[t]  ~  dnorm(mu[t], tau)
    
    # Extracting actual WN value
    e[t]  <- y[t] - mu[t]
  }


  ############################################################
  ######################## PREDICTION ########################
  ############################################################
  
  # 1. Bootstrappingthe out-of-sample predictor
  e_pred[1] ~  dnorm(0, tau)
  mu_out[1] <- m0 + beta * y[N_train] + alpha * e[N_train]
  yp_out[1] ~  dnorm(mu_out[1], tau)
  
  # 2. Recursive step of the out-of-sample predictor
  for (t in 2:N_test) {
    mu_out[t] <- m0 + beta * yp_out[t-1] + alpha * e_pred[t-1]
    yp_out[t] ~  dnorm(mu_out[t], tau)
    e_pred[t] ~  dnorm(0, tau)
  }
  
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################
  
  m0    ~ dnorm(0.0, 1.0E-4)
  beta  ~ dunif(-1, 1)
  alpha ~ dunif(-1, 1)
  tau   ~ dgamma(0.01, 0.01)
  
  # Defining converstion from precision to variance here
  sigma2 <- 1 / tau
}
"
```

## GDP time series

We will, at first, start off with some $AR(n)$ models, with increasing values of $n$. We will use the obtained results ot decide whether or not the selected model may apply to the problem at hand, and we will also evaluate which regressive order is best.

```{r}
# Metrics Dataframe
metrics_df_GDP <- data.frame(
  Model = character(),
  DIC   = numeric(),  
  WAIC  = numeric(), 
  BIC   = numeric(),
  MSE   = numeric(),  
  MAE   = numeric(), 
  MAPE  = numeric(),
  SMAPE = numeric(),  
  MASE  = numeric(),
  stringsAsFactors = FALSE
)

metrics_df_CPI <- data.frame(
  Model = character(),
  DIC   = numeric(),  
  WAIC  = numeric(), 
  BIC   = numeric(),
  MSE   = numeric(),  
  MAE   = numeric(), 
  MAPE  = numeric(),
  SMAPE = numeric(),  
  MASE  = numeric(),
  stringsAsFactors = FALSE
)

```

```{r}
to_save <- c(
  "alpha",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "loglik"
)

# Creating placeholder to store outputs
o <- list()

# Trying our AR(n) from n=1 to max_order
max_order <- 8
for (order in 1:max_order) {
  data_list <- list(
    "y"=series_GDP,
    "order"=order,
    "N_train"=N_train,
    "N_test"=N_test
  )
  
  # Perform simulations and visualise output
  output <- fit_JAGS(
    modelAR_string,
    data_list,
    to_save
  )
  visualise(x_axis, series_GDP, output, title=glue("GDP series, AR({order})"))
  
  if (order == 1) {
    o = output
  }
  
  mets <- compute_metrics(output, series_GDP, N_train, k=order+2)
  metrics_df_GDP <- rbind(
    metrics_df_GDP,
    data.frame(
      Model = paste0("AR(",order,")"),
      DIC   = mets$DIC,
      WAIC  = mets$WAIC,
      BIC   = mets$BIC,
      MSE   = mets$MSE,
      MAE   = mets$MAE,
      MAPE  = mets$MAPE,
      SMAPE = mets$SMAPE,
      MASE  = mets$MASE,
      stringsAsFactors = FALSE
    )
  )
}
```

```{r}
# Extract only AR(1)
output <- o

# Inspecting results
parameters <- c("alpha", "m0")
mcmc_trace(output$samples, pars=parameters)
mcmc_areas(output$samples, pars=parameters, prob=0.95)

print(output)
```

We will now also fit an MA(1) model to evaluate whether it is a better model.

```{r}
to_save <- c(
  "beta",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "loglik"
)
data_list <- list(
  "y"=series_GDP,
  "N_train"=N_train,
  "N_test"=N_test
)

# Perform simulations and visualise output
output <- fit_JAGS(
  modelMA_string_predictor,
  data_list,
  to_save
)

visualise(x_axis, series_GDP, output, title="GDP series, MA(1)")

mets <- compute_metrics(output, series_GDP, N_train, k=3)
metrics_df_GDP <- rbind(
  metrics_df_GDP,
  data.frame(
    Model = "MA(1)",
    DIC   = mets$DIC,
    WAIC  = mets$WAIC,
    BIC   = mets$BIC,
    MSE   = mets$MSE,
    MAE   = mets$MAE,
    MAPE  = mets$MAPE,
    SMAPE = mets$SMAPE,
    MASE  = mets$MASE,
    stringsAsFactors = FALSE
  )
)
```

```{r}
# Inspecting results
parameters <- c("beta", "m0")
mcmc_trace(output$samples, pars=parameters)
mcmc_areas(output$samples, pars=parameters, prob=0.95)

print(output)
```

```{r}
to_save <- c(
  "alpha",
  "beta",
  "m0",
  "sigma2",
  "yp_in",
  "yp_out",
  "loglik"
)
data_list <- list(
  "y"=series_GDP,
  "N_train"=N_train,
  "N_test"=N_test
)

# Perform simulations and visualise output
output <- fit_JAGS(
  modelARMA_string,
  data_list,
  to_save
)

visualise(x_axis, series_GDP, output, title="GDP series, ARMA(1, 1)")

mets <- compute_metrics(output, series_GDP, N_train, k=2+2)

metrics_df_GDP <- rbind(
  metrics_df_GDP,
  data.frame(
    Model = "ARMA(1,1)",
    DIC   = mets$DIC,
    WAIC  = mets$WAIC,
    BIC   = mets$BIC,
    MSE   = mets$MSE,
    MAE   = mets$MAE,
    MAPE  = mets$MAPE,
    SMAPE = mets$SMAPE,
    MASE  = mets$MASE,
    stringsAsFactors = FALSE
  )
)
```
## VAR

Finally, we will to fit generic order VAR(n) models

```{r}
modelVAR_string = "
model {

  ############################################################
  ######################## LIKELIHOOD ########################
  ############################################################
  
  mu[1:2,1:order] = y[,1:order]
  yp_in[1:2,1:order] = y[,1:order]
  
  for (t in (order+1):N_train) {
    mu_tmp[1:2, t, 1] <- m0
    for (j in 1:order) {
      mu_tmp[1:2,t,j+1] <- mu_tmp[1:2, t, j] + A[,,j] %*% y[,(t-j)]
    }
    mu[1:2,t]   <- mu_tmp[1:2,t,order+1]
    y[1:2,t]     ~ dmnorm(mu[1:2,t], tau)
    yp_in[1:2,t] ~ dmnorm(mu[1:2,t], tau)

    # Compute univariate log-likelihood
    loglik_gdp[t]  <- logdensity.norm(y[1,t], mu[1,t], 1 / sigma2[1,1])
    loglik_infl[t] <- logdensity.norm(y[2,t], mu[2,t], 1 / sigma2[2,2])
  }
  
  for (t in 1:order) {
    loglik_gdp[t]  <- logdensity.norm(y[1,t], mu[1,t], 1 / sigma2[1,1])
    loglik_infl[t] <- logdensity.norm(y[2,t], mu[2,t], 1 / sigma2[2,2])
  }

  ############################################################
  ######################## PREDICTION ########################
  ############################################################
  
  z[1:2,1:order] = y[,(N_train-order+1):N_train]
  
  for (t in (order+1):(order+N_test)) {
    mu_z[1:2, t, 1] <- m0
    for (j in 1:order) {
      mu_z[1:2,t,j+1] <- mu_z[1:2, t, j] + A[,,j] %*% z[,(t-j)]
    }
    z[1:2,t] ~ dmnorm(mu_z[1:2,t,order+1], tau)
  }
  
  yp_out[1:2,1:N_test] = z[1:2,(order+1):(order+N_test)]
  
  ############################################################
  ########################## PRIOR ###########################
  ############################################################

  for (i in 1:2) {
    for (j in 1:2) {
      for (o in 1:order) {
        # A[i, j, o] ~ dunif(-1.2, 1.2)
        A[i, j, o] ~ dnorm(0, 10)
      }
    }
  }
  
  tau     ~ dwish(R, 3) # dwish(ScaleMatrix... Identity should be ok, df)
  m_gdp   ~ dnorm(0.0, 1.0E-4)
  m_infl  ~ dnorm(0.0, 1.0E-4)
  m0     <- c(m_gdp, m_infl)
  
  sigma2 <- inverse(tau)
  
  ############################################################
  ######################## EXTRACTION ########################
  ############################################################

  # Expose them for output in R
  loglik_gdp_out   <- loglik_gdp
  loglik_infl_out  <- loglik_infl

  yp_in_gdp   <- yp_in[1,]
  yp_in_infl  <- yp_in[2,]
  yp_out_gdp  <- yp_out[1,]
  yp_out_infl <- yp_out[2,]

}"
```

```{r}
visualise_VAR = function(
  x_axis,               # Values to be displayed on the x-axis of the plots
  original_series_gdp,  # Original GDP time series with all samples
  original_series_infl, # Original INFLATION time series with all samples
  output,               # Output from VAR JAGS, with in-sample and out-of-sample predictions
  title="",             # Title to display on the graphs
  keep=5                # Training sampled to keep in the test graph
) {
  
  output_gdp = list()
  output_gdp$mean$yp_in   = output$mean$yp_in_gdp
  output_gdp$q2.5$yp_in   = output$q2.5$yp_in_gdp
  output_gdp$q97.5$yp_in  = output$q97.5$yp_in_gdp
  output_gdp$mean$yp_out  = output$mean$yp_out_gdp
  output_gdp$q2.5$yp_out  = output$q2.5$yp_out_gdp
  output_gdp$q97.5$yp_out = output$q97.5$yp_out_gdp
  output_gdp$q25$yp_out   = output$q25$yp_out_gdp
  output_gdp$q75$yp_out   = output$q75$yp_out_gdp
  output_gdp$mean$m0      = output$mean$m_gdp
  
  output_infl = list()
  output_infl$mean$yp_in   = output$mean$yp_in_infl
  output_infl$q2.5$yp_in   = output$q2.5$yp_in_infl
  output_infl$q97.5$yp_in  = output$q97.5$yp_in_infl
  output_infl$mean$yp_out  = output$mean$yp_out_infl
  output_infl$q2.5$yp_out  = output$q2.5$yp_out_infl
  output_infl$q97.5$yp_out = output$q97.5$yp_out_infl
  output_infl$q25$yp_out   = output$q25$yp_out_infl
  output_infl$q75$yp_out   = output$q75$yp_out_infl
  output_infl$mean$m0      = output$mean$m_infl
  
  visualise(x_axis, original_series_gdp, output_gdp, title, keep)
  visualise(x_axis, original_series_infl, output_infl, title, keep)
}
```

```{r}
R = matrix(c(1, 0, 0, 1), nrow=2, byrow=TRUE)
y = matrix(c(series_GDP, series_CPI), nrow=2, byrow=TRUE)

order=10

to_save <- c(
  "A",
  "m_gdp",
  "m_infl",
  "sigma2",
  "yp_in_gdp",
  "yp_in_infl",
  "yp_out_gdp",
  "yp_out_infl",
  "loglik_gdp",   # marginal log-likelihood for GDP
  "loglik_infl"   # marginal log-likelihood for CPI
)

data_list <- list(
  "y"=y,
  "R"=R,
  "order"=order,
  "N_train" = N_train,
  "N_test" = N_test
)

jags_output_VAR = fit_JAGS(
  modelVAR_string,
  data_list,
  to_save,
  n_iter=6000,
  n_adapt=1000,
  n_chain=1,
  n_burnin=2000)

visualise_VAR(
  x_axis,
  series_GDP,
  series_CPI,
  jags_output_VAR,
  title=glue("VAR({order})"),
  keep=5
)


# Wrapper for GDP-only metrics
output_gdp <- list(
  mean      = list(yp_out = jags_output_VAR$mean$yp_out_gdp),
  sims.list = list(loglik  = jags_output_VAR$sims.list$loglik_gdp),
  DIC       = compute_dic_marginal(jags_output_VAR$sims.list$loglik_gdp)
)

# Wrapper for CPI-only metrics
output_infl <- list(
  mean      = list(yp_out = jags_output_VAR$mean$yp_out_infl),
  sims.list = list(loglik  = jags_output_VAR$sims.list$loglik_infl),
  DIC       = compute_dic_marginal(jags_output_VAR$sims.list$loglik_infl)
)


# Compute univariate metrics for GDP and CPI
mets_VAR_gdp  <- compute_metrics(output_gdp,  series_GDP, N_train, k = 4*order + 2 + 3)
mets_VAR_infl <- compute_metrics(output_infl, series_CPI, N_train, k = 4*order + 2 + 3)

metrics_df_GDP <- rbind(
  metrics_df_GDP,
  data.frame(
    Model = paste0("VAR_univar_GDP(", order, ")"),
    DIC   = mets_VAR_gdp$DIC,
    WAIC  = mets_VAR_gdp$WAIC,
    BIC   = mets_VAR_gdp$BIC,
    MSE   = mets_VAR_gdp$MSE,
    MAE   = mets_VAR_gdp$MAE,
    MAPE  = mets_VAR_gdp$MAPE,
    SMAPE = mets_VAR_gdp$SMAPE,
    MASE  = mets_VAR_gdp$MASE,
    stringsAsFactors = FALSE
  )
)

metrics_df_CPI <- rbind(
  metrics_df_CPI,
  data.frame(
    Model = paste0("VAR_univar_CPI(", order, ")"),
    DIC   = mets_VAR_infl$DIC,
    WAIC  = mets_VAR_infl$WAIC,
    BIC   = mets_VAR_infl$BIC,
    MSE   = mets_VAR_infl$MSE,
    MAE   = mets_VAR_infl$MAE,
    MAPE  = mets_VAR_infl$MAPE,
    SMAPE = mets_VAR_infl$SMAPE,
    MASE  = mets_VAR_infl$MASE,
    stringsAsFactors = FALSE
  )
)
```

## Metrics

```{r}
# GDP table
kable(
  metrics_df_GDP,
  caption = "Univariate model metrics — GDP series",
  digits  = c(1,1,1,1,3,3,1,1,3),
  format  = "html",
  table.attr = 'cellpadding="10" cellspacing="0"'
)

# CPI table
kable(
  metrics_df_CPI,
  caption = "Univariate model metrics — CPI series",
  digits  = c(1,1,1,1,3,3,1,1,3),
  format  = "html",
  table.attr = 'cellpadding="10" cellspacing="0"'
)
```

PAY ATTENTION:

The MAPE exceeds 100% because the series (quarterly or monthly GDP growth rates) has actual values (denominators) that are very small or close to zero in some periods, which amplifies the percentage.
